# Dealing With Bad Bots
This repository contains a collection of resources, coordinated by the *[Dealing With Bad Bots COAR Task Group](./Task%20Group/)*, to aid repository managers in dealing with an emergent problem.


> [!NOTE]
> Repositories are dedicated to providing open access to information, and we want to encourage legitimate access to this information - **both by human users and machines alike** - in a spirit of mutual benefit. This is increasingly challenged by the recent and explosive growth of the activity of (often badly behaved) web crawling systems.


## Context

Repositories are dedicated to providing open access to information, and we want to encourage legitimate access to this information - **both by human users and machines alike** - in a spirit of mutual benefit. This is increasingly challenged by the recent and explosive growth of the activity of (often badly behaved) web crawling systems.

## The problem

In brief, the problem has two parts:

1. Open Access repositories are reporting a rapid increase in the number of remote systems which are accessing them, with the apparent intention of harvesting or "scraping" as much content as possible from the repository. Many of these systems are ignoring established protocols such as [robots.txt](https://en.wikipedia.org/wiki/Robots.txt) and general networks etiquette, resulting in the repository becoming overwhelmed with an unreasonably high volume of network requests. There are already reports of repositories having been brought down by such activity. We call these badly behaved remote systems *bad bots*.
2. Some of the measures which might reasonably be taken by a repository to mitigate the *bad bots* problem have the potential to impede access to the repository by legitimate, and well behaved remote systems - or even by human users.

